1、验证反向传播模型时，用不加正则项的来验证，无误后再用加正则项的模型开始训练。
2、神经网络的隐藏层类似于一个完成非线性变换到线性变换映射的过程，比如说仅用and or not是一步实现不了异或的，但是如果我们加入了中间层缓冲，就能轻而易举的实现异或了。
3、最后一层误差是al-y 代表预测值和理想值的差距，当x无限小时近似于x在该点斜率，近似于x的导数，也即是x的梯度

4、神经网络的代价函数需要计算所有实例的所有输出单元的输出。我们可以看成是m个实例的输出，每个输出都是一个k维向量（其中k是类别的数量）。这样就相当于是计算一个m*k矩阵的所有元素的和。另一方面，由于通常我们的输出都仅仅是一个实数。我们需要将这个实数展开成一个k维向量，直接y1=ones(k,1),y1(y(i))=1即可，i是指第几个实例。

5、当计算反向传播的θ梯度时，记得要把下一层的δ0给去掉!!,因为下一层的δ0时新加的，和本层的任何一个θ都没有关系，而且若是多出一个δ0将会导致矩阵乘法出错。

6、然而计算θ的梯度时，当前层的θ0也是要计算的，也即是梯度矩阵△要算上θ0，别忘了。

7、正则也是，正则项是除去当前层θ0，将其变为0即可。

8、神经网络步骤：
首先，先把前向传播和反向传播的模型搭建起来。
然后，计算代价函数和梯度矩阵的公式。
再然后，搭建一个小型神经网络和准备一组测试数据，利用梯度检测验证模型有没有错。
接着，使用参数随机化先生成一组随机参数θ。
紧接着，使用前向传播得到一个预测值a3。
再接着，根据这个预测值和实际值的误差反向传播得到梯度矩阵。
最后，重复m次计算m次实例的平均误差梯度矩阵。
最最后，不停训练使用梯度下降或者其他高级优化方法得到使代价函数最小的θ值。

9、一般接受的θ参数都是列向量，我们需要将不同层θ矩阵合并成一个向量，并使用reshape函数将其重新展开。


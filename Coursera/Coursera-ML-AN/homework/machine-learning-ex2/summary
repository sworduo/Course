gmoid(x*theta)别搞错了，而且他的含义也别搞错。
然后就是老套的计算代价函数和梯度下降了。
注意有不止一种方法求代价函数最小值，我们可以只计算代价函数和偏导项，然后用高级优化函数去求解theta

还有正则化，
共有两种，一种是范数1，可以弄稀疏矩阵，一种范数2，可以放置过拟合（其实就是使得theta值尽可能少）

当我们只有两个特征x1 x2，但是数据集的决策边界又很像园或者多项式时，我们可以添加高次幂项，注意添加后要对实例做相应的处理。

别忘了添加x0恒为1！

正则化记得theta0和其他theta的梯度变化要分开。
因为我们用正则化的目的是降低无效特征的影响，而θ0不对应任何特征，所以可不作处理。（就算做处理求到后也为0其实）。

octave end的妙用 contour图的妙用，find的妙用还不错哦

画决策边界时，需要注意的是，当只有两个特征时，我们可以强行算出来 θ0+θ1x1+θ2x2=0把x2当y可直接画图像。
然而当不止一个特征时，我们可以用linspace函数随机生成一组x1 x2，然后带进函数生成一组实例，实例为0的哪些项组成了我们的决策边界，
也即是我们把此时实例为0那些项的轮廓图画出来就行了。
 


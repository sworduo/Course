#	KNN:
##	实现KNN分类器:
*	首先给定训练集X_train,y_train初始化分类器  
*	然后给定测试数据，计算距离矩阵
*	根据距离矩阵计算找到每个测试数据最近的K个点对应的类别
*	通过投票选出该测试数据的类别
*	得到knn分类器模型后，使用交叉验证来判定哪个k值最适合
###	实现细节:
*	对于不通过循环实现距离矩阵，则是应用到公式(x1-x2)^2=x1^2-2x1x2+x2^2,有一点需要注意的是，对于函数np.sum(x,axis=1)，虽然计算的是矩阵x每一行的和，但是其返回值却是一个行向量！！！如果想要返回值是一个列向量，那么需要加一个参数np.sum(x, axis=1, keepdims=True)即可。
*	对于交叉验证中，使用np.array_aplit后返回的是列表，列表每一个下标指向切片后的数组，此时使用vstack时直接相加即可。
______
#	SVM:
##	实现svm分类器:
*	SVM分类器里里面的梯度更新，是基于公式max(0, XW_j-W_i+1),当其大于0时，也就是说loss不为0时，公式变为XW_j-W_i+1,此时求对W_j的梯度其实就是X，同理，对W_i的梯度是-X，注意函数返回的是梯度dW,而不是更新后的参数W。
*	用向量化实现梯度求解时，margin矩阵第i行不为0的元素即为分类错误的元素，这时候第i行的y_i要更新的梯度个数就等于这一行不为0元素的个数和。又或者说，margin里，margin[1][1]=1代表第一个梯度要加一个x1（第一个实例），margin[2][1]=1代表第一个梯度要加一个x2，同理margin[i][j]=1代表第j个梯度要加1个实例xi，这样的话第j梯度要加的数值就是margin第j列和所有实例的线性组合，所以直接X.T*MARGIN即可。  
*	注意，这里的SVM不是常规意义上的大间距分类器，而是指其衍生出来的这也损失函数，用此损失函数去更新梯度。
______
#	softmax:
##	softmax损失函数:
*	softmax是将所有输出单元的输出先进行一次指数，然后计算此时输出单元和所有输出单元和的商，选择最大的一个作为输出。假设输出是{y1,y2,...,yn}，那么每个单元真正的输出是pi=e^yi/(e^y1+e^y2+...+e^yi+...+e^yn)。此时损失函数是-yi*logpi。此时分i=j和i!=j两种情况计算此时的梯度，也就是-yi*logpi对wi,wj的梯度即可。经推导后可以得到，i=j时dwi=xt(pi-1);i!=j时dwj=xt*pj。这是对于同一个实例xt的不同输出单元的更新,i是这个实例xt所对应的标签。  
*	可以参考[这里](https://zhuanlan.zhihu.com/p/25723112)
______
#	softmax和svm的差别：
* 	当softmax每一次每个类别的参数矩阵W都要更新，而svm只需要更新错误分类的参数向量，而那些分类正确也就是sj-si+1<0的类别的参数向量并不会更新。
*	svm比softmax要稳定一点。因为svm只看重那些错误分类的项而不会关注正确分类的项，也就是说如果错误分类的预测值有改变，比如从-3改变到-1，可能也并不会影响输出，因为假设si=2的话,-1-2+1仍然小于0，但是若是采用softmax作为损失函数，那么此时正确类别的输出值会有较为明显的改变。
______
#	两层神经网路:
##	前向传播:
*	使用softmax损失函数，可以点击[这里](https://blog.csdn.net/u014313009/article/details/51045303)看相关推导。当softmax用于神经网络时，需要另外对阈值b求导，此时同样的，分为i=j和i!=j两种情况对b1,b2,b3,...,bn求导。要注意的是，每一个输出单元的b是不一样的，推导的时候要注意这里。具体来说假设输出层输入是a，那么a1=w1x+b1,a2=w2x+b2,a3=w3x+b3,...,an=wnx+bn。
##	反向传播：
	反向传播这一部分代码建议把神经网络的图画出来，这里有几个需要注意的点:
*	首先，激活函数是ReLU函数，所以，隐藏层节点输出和输入的导数是1或者0。
*	其次，代价函数是softmax函数的log求和，要求代价函数对隐藏层输出的导数，可以先求代价函数对输出层输出也就是wx+b的导数（其实是pi-yi),然后一层一层的迭代回去就可以了。  
*	求梯度时，要将最终得到的概率矩阵除以N，因为每次反向传播时，是拿N个样本的平均值来更新参数的。如果不除以N，就相当于是直接经过N次训练，这样会放大噪声。（本来N个样本平均就是用来减少噪声的）  
##	预测
*	预测的时候，分数最大就等于概率最大，所以只要计算出最终分数并选择分数最大的那个类别就好了。不需要再求指数然后归一化成概率来判断。  
______
#	feature:
*	执行第三个单元（也就是抽取特征那个单元）时，会报一个类型错误的提示，可参考[这里](https://blog.csdn.net/bigdatadigest/article/details/79083210)。
*	关于梯度方向直方图特征和HSV特征，可以参考[这里](https://www.jianshu.com/p/395f0582c5f7)
